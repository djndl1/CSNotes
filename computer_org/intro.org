* The Books

The book "Computer Organization and Design" is intended for software designers
to understand the principles of basic hardware techniques at work in a system
while "Computer Architecture: A Quantitative Approach" aims to create realistic
design experience for a more professional audience using quantitative
methodologies instead of a descriptive approach. RISC-V is used as an example
ISA due to its open-source nature and hardware/emulator availability.

* Why Learn Computer Organization and Architecture

Based on the current state of the computing field and appreciation of the organizational paradigms.

Modern computer technology requires professionals to understand
both hardware and software:

- The central ideas in computer organization and design are the same regardless of hardware or software.

- Recent developments in computing, especially multicore microprocessors have
  made programmers no longer able to ignore the underlying parallel nature of
  the hardware they are programming for some time: the parallel nature of
  processors and the hierarchical nature of memories, how the software and
  hardware interact, what determines the performance of a program, the
  techniques used by the hardware designer to improve performance (for more read
  Computer Architecture: A Quantitative Approach), the founding principles of
  modern computing.

- Improving program performance should be a scientific procedure driven by insight and analysis rather than a complex process of trial and error.

From a programmer's perspective, program performance matters much. It depends on a combination of

- algorithm

- programming language, compiler and architecture

- processor and memory system

- I/O system (hardware and OS).


* Recent Developments

- The slowing of Moore's Law

- The rise of Domain-Specific Architectures (DSA): e.g. Google's TPU

- Microarchitecture as a security attack surfaces: speculative out-of-order
  execution and hardware multithreading make timing based side-channel attacks
  practical.

- Open instruction sets and open source implementations: e.g. RISC-V

- The re-virtualization of the IT industry and cloud computing.

- Massive increase of personal mobile devices.

* Basic Ideas in Design

- Use *abstractions* to simplify design;
  + Both hardware and software consists of hierarchical layers using abstraction.
  + e.g. software architecture; high-level language to low-level assembly and
    machine code; ISA as the interface between software and hardware; ABI; API;
    interface and implementation

- Making the *common case* fast tends to enhance performance better than
  optimizing the rare case;
  + e.g. =x0= in RISC-V, special immediate instructions in RISC-V

- Performance via *parallelism*;

- Performance via *pipelining*: e.g. branch prediction

- Performance via *prediction*: better to ask for forgiveness than to ask for
  permission. It can be faster on average to guess and start working rather than
  wait until it is known for sure.

- *Hierarchy* of Memories: conflicting demands of fast large and cheap memory
  are met with memory hierarchy.

- Dependability via *Redundancy*

* The Big Picture

*Functions*: inputting data, outputting data, storing data, processing data,

*The Five Classic Components*:

- /input/: LCD, touchscreens

- /output/:

- /memory/: memory hierarchy (cache, primary memory, secondary memory)

- /datapath/: data processing units, registers and buses.

- /control/: commands the datapath, memory and I/O devices according to the
  instructions of the program.

The content on computer organization resolves around these five basic components.

*Instruction Set Architecture*: the interface between hardware and low-level software.

*Network*: communication, resource sharing, nonlocal access

* Performance

Due to the scale and
intricacy of modern software systems and the wide range of performance
improvement techniques employed by hardware designers, performance assessment
has been difficult. There are limitations of various performance measurements
and understanding them is crucial in selecting a computer.

The definition of performance itself is multidimensional. Normal PC users care
about *response/execution time* while *throughput/bandwidth* matters more for
datacenters.

Performance may be defined as the reciprocal of execution time. However, there
is a difference between /elapsed time/ (/wall clock time/) and actual time used
by the CPU (/CPU time/). Furthermore, CPU time can be divided into the user
spent on the user program (/user CPU time/) and the time spent in the OS on
behalf of the user program (/system CPU time/). This distinction is made so that
user programs' performance is assessed separately from the OS'. /System
performance/ is measured when the OS is unloaded. CPU time is not the only
metric of performance: the actual performance requirements of programs vary,
throughput, response time or a complex combination of the two.

$$
\dfrac{\text{Seconds}}{\text{Program}} =
\dfrac{\text{Instructions}}{\text{Program}} \times \dfrac{\text{Clock
 cycles}}{\text{Instructions}} \times \dfrac{\text{Seconds}}{\text{Clock cycles}}
$$

Performance may be measured at different levels:

- (Clock cycle) The CPU time of a program is determined by the *clock cycles* to execute the program (may not be fixed due to various CPU design techniques) and the *clock rate* of the CPU (fixed once the CPU is chosen).

- (Instruction level) the clock cycles used by a program equals to the number of instructions for a program (fixed on a given ISA) multiplied by *the average time per instruction* (*CPI*, or reciprocal *IPC* to measure *instruction performance*)

$$
\text{CPU time} = \dfrac{\text{Instruction count} \times \text{CPI}}{\text{Clock rate}}
$$

While  comparing different ISAs, even if the clock rates are equal,
the instruction counts and CPIs may be different, determined by the /algorithm/,
the /programming language/, the /compiler/ in use, the /ISA/.

An alternative to time as a performance metric is *MIPS* (/Million Instructions
Per Second/, an instruction execution rate):

$$
\text{MIPS} = \dfrac{\text{Instruction Count}}{\text{Execution time} \times
10^{6}} = \dfrac{\text{Clock Rate}}{\text{CPI} \times 10^{6}}
$$

MIPS does not take into consideration /the capabilities of instructions/ and
varies between programs on the same computer.

Execution time remains *the only valid and unimpeachable measure of performance*.

** Performance Measurement

Either use a workload (a set of programs to evaluate the performance of a
computer) or a set of /benchmarks/ (programs specifically chosen to measure performance).

- /SPEC/ (System Performance Evaluation Cooperative): a set of integer
  benchmarks and float-point benchmarks .

* Power Consumption

$$
\text{Power} \propto 1/2 \times \text{Capacitive Load} \times \text{Voltage}^{2}
\times \text{Frequency switched}
$$


For the recent decades, frequency has increased more than 1000 times, while voltage
has decreased from 5V to 1V, resulting in only about 30 times in power.
Further lowering of the voltage appears to make the transistors too leaky:
leakage current flows even when a transistor is off, typically responsible for
40% of the energy consumption. Computer designers have hit the power wall:
power consumption, distribution across the chip and heat dissipation is
preventing further performance increase. Increasing performance simply by
increasing the clock rate is no longer feasible.

Typical computers are not energy-proportional, that is, the power consumption at
low workload, compared to its peak power consumption, is much higher than its
workload compared to the full workload. Conserving power while trying to
increase performance has forced the hardware industry to switch to multicore microprocessors.


* Multiprocessing: Rethinking Hardware/Software Interface

Due to the power wall, multicore processors are introduced to overcome the
performance limit of unicore processors. This somehow broke the previous
hardware-software interface that programmers have to rewrite programs to take
advantage of multiple processors.

Writing parallel programs requires more than functional correctness but also
understanding of performance programming. It also means more work for the
programmers: scheduling, load balancing, time for synchronization, overhead for
communication between the parties. Modern computer design focuses heavily on
parallelism at various levels: instruction-level parallelism (pipelining,
prediction, out-of-order execution), subword parallelism, multiprocessing.

* Amdahl's Law

$$
\text{Execution time after improvement} = \dfrac{\text{Execution time affected by
improvement}}{\text{Amount of improvement}} + \text{Execution time unaffected}
$$

#+begin_quote
"the overall performance improvement gained by optimizing a single part of a
system is limited by the fraction of time that the improved part is actually used"
#+end_quote

There is just so much performance we can improve by improving one aspect of a computer.

* Basic Computer Ideas and History

** Stored Program

Programs are stored as data (numbers) and is later loaded and executed by a computer, which contrasts with systems that stored the program instructions with plugboards or similar mechanisms or systems that are hardwired and unprogrammable.

ENIAC might be the first operational general-purpose electronic computer, but it
used wires and switches for programming initially (later modified into a stored program system). A stored program computer was not a completely
new idea in the 1940s. Von Neumann crystallized the idea and turned into an
influential internal memo. Maurice Wilkes of Cambridge University built the first
full-scale operational stored-program computer EDSAC.

On a large scale, the ability to treat instructions as data is what makes
assemblers, compilers, linkers, loaders, and other automated programming tools possible. It makes "programs that write programs" possible.

** Von Neumann Computer

*** Key Design Principles

**** EDVAC

- Large addressable read/write memory

- Binary number representation: more efficient in storage than decimal

**** Von Neuman Architecture Paradigm

- separate organs for storage, arithmetic and control

- special purpose registers: accumulator, instruction register, program counter

- program executed from fast memory: program instructions should be held in
  numbere3d memory locations, randomly accessible at high speed.

- Fully interchangeable memory

- Program loadable from external media: it should be possible to read the
  program rapidly into memory from some external medium.

**** Modern Programming Paradigm

- Sequential atomic instructions: separate, atomic instructions to be processed
  in a sequence as a program

- Automated jump

- Instructions operating on variable addresses: it should be possible to vary
  the address part of an instruction, nowadays realized as indexed or indirect addressing.
  + e.g. apply the same code to different data elements; support subroutines for return values.


** Harvard architecture

*** The Original Harvard Machines

This term was coined in the 1970s retrospectively to describe the Harvard
machines designed by the Harvard Computing Laboratory.
Today it is applied to machines with a single main memory but with separate
instruction and data caches. For these machines, the CPU accesses instruction
and data simultaneously (not for the original Harvard machines). The term has
come to represent a supposedly different and superior architecture compared to the Von
Neumann architecture.

The original Harvard machines, especially Mark III and IV, implemented most of
EDVAC ideas. Mark III and IV had separate memory for instructions and data and
instruction memory was no writable by other instructions. The design of separate
stores for data and instruction was most likely an optimization: 16-digit
decimals were used for data and a 38-bit format was used for instructions, not
for protection of instructions and data. Modern time-sharing computers require
dynamic loading of programs, i.e. writable instruction memory. Neither the
original Harvard machines nor the EDVAC design could accomplish such tasks.

*** Microcontrollers, for Which the Term Was Invented Later

Microcontrollers arised in the late 1970s, where data is stored in a persistent
ROM, hardwared to the PC and instruction register while variable data are stored
in the RAM connected to an address and data bus. This design resulted from the
simplicity of less multiplexing on the buses and different widths of ROM and RAM
buses. In this architecture, the next instruction can be fetched without waiting for
dependent data to be fetched.

The term "Harvard Architecture" was cointed at this time to describe this new
architecture, even though the connection to the original Harvard machines is
tenuous. For microcontrollers, this separation was not a choice but a must. Also
the original Harvard machines cannot access data and instructions simultaneously.

*** Split caches in RISC Microprocessors

Since instruction's tend to static, caches are split for instructions and data
to be optimized for each. This design is often referred to as "Modified Harvard
Architecture", a misnomer as it has nothing to oo with the original Harvard machines. This design is not limited to classical RISC machines: the P5
microarchitecture from Intel has two 8KB caches for code and data separately.
